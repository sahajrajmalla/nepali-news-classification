{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IT MEET 2022: Nepali News Classifier using HuggingFace","metadata":{}},{"cell_type":"markdown","source":"## Import required datasets","metadata":{}},{"cell_type":"code","source":"initial_dataset = pd.read_csv(\"../input/text-it-meet-22/train.csv\")\ninitial_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing dataset","metadata":{}},{"cell_type":"markdown","source":"### De-duplication","metadata":{}},{"cell_type":"code","source":"before_de_duplicate_shape = initial_dataset.shape\nprint(f\"The dataset shape before de-duplication was {before_de_duplicate_shape}\")\n\n# de-duplication\ninitial_dataset.drop_duplicates(inplace=True)\n\nafter_de_duplicate_shape = initial_dataset.shape\nprint(f\"The dataset shape after de-duplication was {after_de_duplicate_shape}\")\nprint(f\"The total number of rows that were removed were {before_de_duplicate_shape[0] - after_de_duplicate_shape[0]}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop NaN values","metadata":{}},{"cell_type":"code","source":"before_drop_na_shape = initial_dataset.shape\nprint(f\"The dataset shape before de-duplication was {before_drop_na_shape}\")\n\n# dropping NaN values\ninitial_dataset.dropna(inplace=True)\n\nafter_drop_na_shapee = initial_dataset.shape\nprint(f\"The dataset shape after de-duplication was {after_drop_na_shapee}\")\nprint(f\"The total number of rows that were removed were {before_drop_na_shape[0] - after_drop_na_shapee[0]}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label Encoding","metadata":{}},{"cell_type":"code","source":"# total number of labels\nlen(initial_dataset[\"label\"].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Encoding\n\n# Import label encoder\nfrom sklearn import preprocessing\n  \n# label_encoder object knows how to understand word labels.\nlabel_encoder = preprocessing.LabelEncoder()\n  \n# Encode labels in column 'species'.\ninitial_dataset['label']= label_encoder.fit_transform(initial_dataset['label'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preview of our encoded labels\nle_name_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\nle_name_mapping","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train-validation split","metadata":{}},{"cell_type":"code","source":"# import modules\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n \n# split the dataset\ntrain_dataset, validation_dataset = train_test_split(\n    initial_dataset, test_size=0.1, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving it to csv","metadata":{}},{"cell_type":"code","source":"import csv\n\n# here, random sample of 33K is due to the memory constraint faced using training in Kaggle \ntrain_dataset.sample(33000).to_csv(\"./preprocessed_train.csv\", index=False, quoting=csv.QUOTE_ALL)\nvalidation_dataset.to_csv(\"./preprocessed_val.csv\", index=False, quoting=csv.QUOTE_ALL)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare a dataset for HuggingFace DistilBERT model\n","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndata_files = {\"train\": \"./preprocessed_train.csv\", \"eval\": \"./preprocessed_val.csv\"}\ndataset = load_dataset('csv', data_files=data_files)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preview of our processed dataset\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preview of the format of our validation dataset\npreview_dataset = pd.DataFrame(dataset[\"eval\"])\npreview_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing and downloading Tokenizer and Models from HuggingFace\n\nI have used Suyogyart/nepali-16-newsgroups-classification from huggingface.co which can be found here:\n- Suyogyart/nepali-16-newsgroups-classification -> https://huggingface.co/Suyogyart/nepali-16-newsgroups-classification","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"Suyogyart/nepali-16-newsgroups-classification\")\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"data\"], padding=\"max_length\", truncation=True)\n\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preparing training and validation dataset\n\ntrain_dataset = tokenized_datasets[\"train\"].shuffle(seed=0)\neval_dataset = tokenized_datasets[\"eval\"].shuffle(seed=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\n# downloading model\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Suyogyart/nepali-16-newsgroups-classification\", num_labels=len(initial_dataset[\"label\"].value_counts()), ignore_mismatched_sizes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training hyperparameters","metadata":{"execution":{"iopub.status.busy":"2022-08-12T11:50:33.775802Z","iopub.execute_input":"2022-08-12T11:50:33.776470Z","iopub.status.idle":"2022-08-12T11:50:33.780764Z","shell.execute_reply.started":"2022-08-12T11:50:33.776357Z","shell.execute_reply":"2022-08-12T11:50:33.779815Z"}}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(output_dir=None, evaluation_strategy=\"epoch\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom datasets import load_metric\n\nmetric = load_metric(\"accuracy\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trainer","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training our model\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving our model","metadata":{}},{"cell_type":"code","source":"save_directory = \"./model-aug13-news-classifier-iter-4-33K\"\ntokenizer.save_pretrained(save_directory)\nmodel.save_pretrained(save_directory)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ziping our model","metadata":{}},{"cell_type":"code","source":"!zip -r model-aug13-news-classifier-iter-4-33K.zip model-aug13-news-classifier-iter-4-33K","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pushing it into AWS s3 bucket ","metadata":{}},{"cell_type":"code","source":"import boto3\n\nAWS_ACCESS_KEY_ID = ''\nAWS_SECRET_ACCESS_KEY = ''\n\ns3 = boto3.resource(service_name = 's3', aws_access_key_id= AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\ns3.meta.client.upload_file(Filename = './model-aug13-news-classifier-iter-4-33K.zip', Bucket = \"realpha-models-registry\", Key = 'model-aug13-news-classifier-iter-4-33K.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}